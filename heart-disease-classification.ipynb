{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart-disease using machine learning\n",
    "\n",
    "This notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n",
    "\n",
    "I am going to take the following steps:\n",
    "1) Problem Definition\n",
    "2) Data\n",
    "3) Evaluation \n",
    "4) Features\n",
    "5) Modelling\n",
    "6) Experimentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data Analysis Libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# This inline command will make the plot outputs appear and be stored within the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "# Importing Models from Scikit Learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importing Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data and Data Exploration (Exploratory Data Analysis or EDA)\n",
    "\n",
    "df = pd.read_csv('data/heart-disease.csv')\n",
    "df.shape    #(rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing features of heart-disease dataset to the Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Create a plot of the crosstab\n",
    "pd.crosstab(df.target, df.sex).plot(kind='bar', color=['salmon', 'lightblue'], figsize=(10, 6))\n",
    "plt.title('Heart Disease Frequency for Sex')\n",
    "plt.xlabel('0 = No Disease, 1 = Disease')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend([\"Female\", \"Male\"])\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age vs Max Heart Rate for Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Create another figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter with positive examples\n",
    "plt.scatter(df.age[df.target==1],\n",
    "            df.thalach[df.target==1],\n",
    "            c='salmon')\n",
    "\n",
    "# Scatter with negative examples\n",
    "plt.scatter(df.age[df.target==0],\n",
    "            df.thalach[df.target==0],\n",
    "            c='lightblue')\n",
    "\n",
    "# Add some helpful info\n",
    "plt.title('Heart Disease in function of Age and Max Heart Rate')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Max Heart Rate')\n",
    "plt.legend(['Disease', 'No Disease'])\n",
    "plt.show()\n",
    "\n",
    "# Check the distribution of the age column with a histogram\n",
    "df.age.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart Disease Frequency per Chest Pain Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "pd.crosstab(df.cp, df.target).plot(kind='bar', figsize=(10, 6), color=['salmon', 'lightblue'])\n",
    "plt.title('Heart Disease Frequency Per Chest Pain Type')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['No Disease', 'Disease'])\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt=\".2f\", cmap=\"YlGnBu\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "\n",
    "# Split data into X and y\n",
    "x = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Split data into train and test sets\n",
    "np.random.seed(42)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we've got our data split into training and test sets, it's time to build a machine learning model.\n",
    "# We'll train it (find the patterns) on the training set.\n",
    "# And we'll test it (use the patterns) on the test set.\n",
    "\n",
    "# We're going to try 3 different machine learning models:\n",
    "# 1. Logistic Regression\n",
    "# 2. K-Nearest Neighbours Classifier\n",
    "# 3. Random Forest Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put models in a dictionary\n",
    "models = {\"Logistic Regression\": LogisticRegression(),\n",
    "          \"KNN\": KNeighborsClassifier(),\n",
    "          \"Random Forest\": RandomForestClassifier()}\n",
    "\n",
    "# Create a function to fit and score models\n",
    "def fit_and_score(models, x_train, x_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Fits and evaluates given machine learning models.\n",
    "    models : a dict of different Scikit-Learn machine learning models\n",
    "    x_train : training data (no labels)\n",
    "    x_test : testing data (no labels)\n",
    "    y_train : training labels\n",
    "    y_test : test labels\n",
    "    \"\"\"\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "    # Make a dictionary to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(x_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(x_test, y_test)\n",
    "    return model_scores\n",
    "\n",
    "model_scores = fit_and_score(models=models, x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Model Comparison\n",
    "model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\n",
    "model_compare.T.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n",
    "\n",
    "Let's look at the following:\n",
    "1. Hyperparameter tuning\n",
    "2. Feature importance\n",
    "3. Confusion matrix\n",
    "4. Cross-validation\n",
    "5. Precision\n",
    "6. Recall\n",
    "7. F1 score\n",
    "8. Classification report\n",
    "9. ROC curve\n",
    "10. Area under the curve (AUC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets tune KNN\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21)\n",
    "\n",
    "# Setup KNN instance\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different n_neighbors\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors=i)\n",
    "    # Fit the algorithm\n",
    "    knn.fit(x_train, y_train)\n",
    "    # Update the training scores list\n",
    "    train_scores.append(knn.score(x_train, y_train))\n",
    "    # Update the test scores list\n",
    "    test_scores.append(knn.score(x_test, y_test))\n",
    "    \n",
    "plt.close(\"all\")\n",
    "    \n",
    "plt.plot(neighbors, train_scores, label=\"Train score\")\n",
    "plt.plot(neighbors, test_scores, label=\"Test score\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with RandomizedSearchCV\n",
    "\n",
    "We're going to tune:\n",
    "* LogisticRegression()\n",
    "* RandomForestClassifier()\n",
    "\n",
    "... using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparameter grid for LogisticRegression\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Create a hyperparameter grid for RandomForestClassifier\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got hyperparameters setup for both models, let's tune them using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune LogisticRegression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model for LogisticRegression\n",
    "rs_log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the random search LogisticRegression model\n",
    "rs_log_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifier()..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model for RandomForestClassifier\n",
    "rs_rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized search RandomForestClassifier model\n",
    "rs_rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with GridSearchCV\n",
    "Since our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different hyperparameters for our LogisticRegression model\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Check the best hyperparameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the grid search LogisticRegression model\n",
    "gs_log_reg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our tuned machine learning classifier, beyond accuracy \n",
    "\n",
    "* ROC curve and AUC score\n",
    "* Confusion matrix\n",
    "* Classification report\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-score\n",
    "\n",
    "... and it would be great if cross-validation was used where possible.\n",
    "\n",
    "To make comparisons and evaluate our trained model, first we need to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with tuned model\n",
    "y_preds = gs_log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Plot ROC curve and calculate AUC metric\n",
    "RocCurveDisplay.from_estimator(estimator=gs_log_reg, \n",
    "                               X=x_test, \n",
    "                               y=y_test); \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Confusion matrix\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "sns.set_theme(font_scale=1.5)\n",
    "\n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True,\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
